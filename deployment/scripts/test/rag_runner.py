from db import get_similar_documents
from llm import ask_llm
from sentence_transformers import SentenceTransformer
import os
from dotenv import load_dotenv

load_dotenv("config.env")

model = SentenceTransformer("all-MiniLM-L6-v2")

def extract_search_topic(question: str) -> str:
    prompt = f"""
Extrae el concepto general de esta pregunta para hacer una bÃºsqueda semÃ¡ntica de pelÃ­culas.

Pregunta: {question}

Devuelve solo una frase corta en inglÃ©s que represente el tema principal. No expliques nada mÃ¡s.
"""
    topic = ask_llm(prompt)
    return topic.strip()


def rag_pipeline(question: str, top_k: int = 3):
    refined_query = extract_search_topic(question)
    print(f"\nğŸ” TÃ³pico extraÃ­do para bÃºsqueda:\n{refined_query}")

    question_embedding = model.encode(refined_query).tolist()
    vector_store = PostgresVectorStore()
    documents = vector_store.get_similar_documents(embedding, top_k=3)
    #documents = get_similar_documents(question_embedding, top_k=top_k)

    print("\nğŸ“š PelÃ­culas similares encontradas:\n")
    for i, doc in enumerate(documents, start=1):
        print(f"{i}. ğŸ¬ TÃ­tulo: {doc['title']}")
        print(f"ğŸ–¼ï¸ Imagen: {doc['image']}")
        print(f"ğŸ§  Similaridad: {doc['similarity']:.4f}")
        print(f"ğŸ“„ Sinopsis:\n{doc['content'][:400]}...\n{'-'*60}")

    # Construir el contexto que se le pasa al LLM
    context = ""
    for doc in documents:
        context += f"""
TÃ­tulo: {doc['title']}
Imagen: {doc['image']}
Sinopsis: {doc['content']}
---
"""

    prompt = f"""
Usa la siguiente informaciÃ³n de pelÃ­culas para responder la pregunta del usuario. Si alguna coincide claramente con lo que se busca, menciona su tÃ­tulo.

InformaciÃ³n:
{context}

Pregunta original:
{question}
    """.strip()

    answer = ask_llm(prompt)
    return answer


if __name__ == "__main__":
    print("ğŸ“¥ Pregunta de usuario:")
    question = input("Pregunta: ")

    answer = rag_pipeline(question)

    print("\nğŸ’¬ Respuesta generada por el LLM:\n")
    print(answer)
